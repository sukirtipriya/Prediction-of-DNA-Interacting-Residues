# -*- coding: utf-8 -*-
"""MLBA_Hakathon_fin

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SKr50EBzZcYaqyl9jx5PxEvdUu70PjMj
"""

#Importing libraries
import glob
import pandas as pd
import numpy as np
import sys, getopt
import tensorflow as tf
import matplotlib.pyplot as plt

# for preprocessing & feature selection
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel

# for Cross valiadtion
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

# for evaluating the model
from sklearn.metrics import accuracy_score
from tqdm import tqdm
from sklearn.metrics import matthews_corrcoef
from sklearn.model_selection import train_test_split
from sklearn import metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from xgboost import XGBClassifier

#For command line operation
def main(argv):
	inputfile = ''
	outputfile = ''
	try:
		opts, args = getopt.getopt(argv,"hi:o:",["ifile=","ofile="])
	except getopt.GetoptError:
		print('script.py -i <test.csv> -o <predict.csv>\nInput file contains features of sequences with labels under the \'Label\' column\nOutput file contains Protein IDs and their subsequent labels\nFor more details please refer to README.txt')
		sys.exit(2)
	for opt, arg in opts:
		if opt in ("-h", "--Help"):
			print('script.py -i <test.csv> -o <predict.csv>\nInput file contains features of sequences with labels under the \'Label\' column\nOutput file contains Protein IDs and their subsequent labels\nFor more details please refer to README.txt')
			sys.exit()
		elif opt in ("-i", "--ifile"):
			inputfile = arg
			print('Taking input from', inputfile)
		elif opt in ("-o", "--ofile"):
			outputfile = arg
			print('Writing output into', outputfile,"\n\n")
	return inputfile,outputfile
if __name__ == "__main__":
    data1,data2=main(sys.argv[1:])
    #print(data1,data2)

#Importing Training data features
Train_dataset = pd.read_csv('/content/drive/MyDrive/MLBA Hakathon/Train_faetures.csv', index_col=None, header=0)
Tr_label = Train_dataset[['Label']]
Tr_data = Train_dataset.loc[:,Train_dataset.columns != 'Label']
Tr_label=np.ravel(Tr_label)
X_trn = Tr_data

#Importing Validation data features
Valid_dataset = pd.read_csv(data1, index_col=None, header=0)
Valid_id = Valid_dataset[['ID']]
Valid_data = Valid_dataset.loc[:, Valid_dataset.columns != 'ID']

#Feature selection
clf = ExtraTreesClassifier(n_estimators=500,random_state=135)
clf = clf.fit(Tr_data, Tr_label)
model = SelectFromModel(clf, prefit=True)
X = model.transform(Tr_data) #38 features are selected from 77
Z = model.transform(Valid_data)


#function for writing the data output
def scoreData(data, model, IDs):
  y_pred = model.predict(data)
  y=pd.DataFrame(y_pred)
  z=pd.DataFrame(IDs)
  res=[z,y]
  result = pd.concat(res, axis=1)
  result.columns =['ID', 'Label'] 
  return(result)

#Suffling and spltting of data set
train_size = int(0.8 * len(X_trn))
train_set_x = X_trn[:train_size]
train_set_y = Tr_label[:train_size]
test_set_x = X_trn[train_size:]
test_set_y = Tr_label[train_size:]

#KNeighbours Classifier
hyperparameters = dict(leaf_size=1, n_neighbors=29, p=1)
#Create new KNN object
knn = KNeighborsClassifier()
knn.fit(train_set_x,train_set_y)
knn_pred=knn.predict(test_set_x)
print("KNeighbours Accuracy:",accuracy_score(test_set_y, knn_pred))

#SVC classifier
clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))
clf.fit(train_set_x, train_set_y)
Pipeline(steps=[('standardscaler', StandardScaler()),('svc', SVC(gamma='auto'))])
svc_pred=clf.predict(test_set_x)
print("SVC Accuracy:",metrics.accuracy_score(test_set_y, svc_pred))

#XGBoost Classifier
xgb = XGBClassifier(max_depth=12,
                        subsample=0.33,
                        objective='binary:logistic',
                        n_estimators=1500,
                        learning_rate = 0.01,
                        early_stopping_rounds=10)
xgb.fit(train_set_x, train_set_y)
# make predictions for test data

y_pred_gb = xgb.predict(test_set_x)
predict = [round(value) for value in y_pred_gb]
accuracy = accuracy_score(test_set_y, predict)
print("XGBoost Accuracy: %.2f%%" % (accuracy * 100.0))

#Ensemble of multiple classfier
models={}
models[0] = RandomForestClassifier(n_estimators = 750, random_state = 42)
models[0].fit(train_set_x, train_set_y)
models[1] = KNeighborsClassifier()
models[1].fit(train_set_x, train_set_y)
models[2] = make_pipeline(StandardScaler(), SVC(gamma='auto'))
models[2].fit(train_set_x, train_set_y)
models[3] = XGBClassifier(max_depth=12, subsample=0.33, objective='binary:logistic', n_estimators=1500, learning_rate = 0.01,early_stopping_rounds=10)
models[3].fit(train_set_x, train_set_y)
#make_pipeline(steps=[('standardscaler', StandardScaler()),('svc', SVC(gamma='auto'))])

#Final prediction by majourity vote
final_test_prediction = []
unique_labels=[1,0]
for sample in test_set_x:
      labels = []
      for m in models.keys():
          pds = models[m].predict([sample])
          labels.append(pds)
      if labels.count(1)>labels.count(0):
          final_test_prediction.append(1)
      else:
          final_test_prediction.append(0)
          
print("Ensemble Accuracy:",accuracy_score(test_set_y, final_test_prediction))

X_tr = np.array(X_trn)
trn_set_x = np.array(train_set_x)
tes_set_x = np.array(test_set_x)
val_data = np.array(Valid_data)

# k-fold cross validation 
kf = KFold(n_splits=5, random_state =1234, shuffle= True )
kf.get_n_splits(X_tr)
#print(kf)
models={}
c=0
for train_index, test_index in kf.split(X_tr):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X_tr[train_index], X_tr[test_index]
    Y_train, Y_test = Tr_label[train_index], Tr_label[test_index]
    models[c] = RandomForestClassifier(n_estimators=500,random_state = 42,n_jobs=-1)
    models[c].fit(X_train, Y_train)
    c+=1
final_test_prediction = [] #Final prediction by majourity vote
unique_labels=[1,0]
for sample in tes_set_x:
      labels = []
      for m in models.keys():
          pds = models[m].predict([sample])
          labels.append(pds)
      if labels.count(1)>labels.count(0):
          final_test_prediction.append(1)
      else:
          final_test_prediction.append(0)
          
print("Final Accuracy:",accuracy_score(test_set_y, final_test_prediction))

#Final prediction by majourity vote
final_trn_prediction = []
unique_labels=[1,0]
for sample in val_data:
      labels = []
      for m in models.keys():
          pds = models[m].predict([sample])
          labels.append(pds)
      if labels.count(1)>labels.count(0):
          final_trn_prediction.append(1)
      else:
          final_trn_prediction.append(0)

y=pd.DataFrame(final_trn_prediction)
z=pd.DataFrame(Valid_id)
res=[z,y]
result = pd.concat(res, axis=1)
result.columns =['ID', 'Label'] 
result.to_csv(data2, index = False)
